Links to some important research papers or links. I plan to add notes as I go through each topic one by one.

### Disentangled representations
* [Quick overview by Google](https://ai.googleblog.com/2019/04/evaluating-unsupervised-learning-of.html)
* [Disentangling Disentanglement in Variational Autoencoders](https://arxiv.org/abs/1812.02833)
* [InfoGAN-CR: Disentangling Generative Adversarial Networks with Contrastive Regularizers](https://arxiv.org/abs/1906.06034)

__Contrastive Coding__:
* [Data-Efficient Image Recognition with Contrastive Predictive Coding](https://arxiv.org/abs/1905.09272)
* [Contrastive Multiview Coding](https://arxiv.org/abs/1906.05849)
* Google disspelling a lot of misconceptions about disentangled representations: [Challenging Common Assumptions in the Unsupervised Learning of Disentangled Representations](https://arxiv.org/abs/1811.12359)

### Memorization in neural networks
* [Blog post by BAIR](https://bair.berkeley.edu/blog/2019/08/13/memorization/)
* [Identity Crisis: Paper](https://arxiv.org/abs/1902.04698)

### Information theory based (unsupervised) learning
* [Invariant Information Clustering](https://arxiv.org/abs/1807.06653) | [__Notes__](https://github.com/vinsis/math-and-ml-notes/blob/master/notes/iic.md)
* [Mutual Information Neural Estimation](https://arxiv.org/abs/1801.04062) | [__Notes__](https://github.com/vinsis/math-and-ml-notes/blob/master/notes/mine.md)
* [Deep Infomax](https://arxiv.org/abs/1808.06670)
* [Learning Representations by Maximizing Mutual Information Across Views](https://arxiv.org/abs/1906.00910)
* How Google invalidated most of the above research: [On Mutual Information Maximization for Representation Learning](https://arxiv.org/abs/1907.13625)

### Links between ResNets and ODEs
* [Beyond Finite Layer Neural Networks](https://arxiv.org/pdf/1710.10121.pdf)
* [Neural Ordinary Differential Equations](https://arxiv.org/pdf/1806.07366.pdf)
* [Invertible ResNets](https://arxiv.org/pdf/1811.00995.pdf)

### Normalizing Flows
* [Detailed hands-on introduction](https://github.com/acids-ircam/pytorch_flows)
* [PyTorch implementations of density estimation algorithms](https://github.com/kamenbliznashki/normalizing_flows)

### Transformers
* [Attention is all you need](https://arxiv.org/abs/1706.03762)
* [Links to papers and code on various transformers by HuggingFace](https://github.com/huggingface/transformers)

### Geometric deep learning
* [Good list of resources](http://geometricdeeplearning.com/)

### Probabilistic programming
* [An introduction to probabilistic programming](https://arxiv.org/abs/1809.10756)

### Miscellaneous
* [Lottery ticket hypothesis](http://news.mit.edu/2019/smarter-training-neural-networks-0506)
* [Zero-shot knowledge transfer](https://arxiv.org/abs/1905.09768)
* [SpecNet](https://arxiv.org/abs/1905.10915)

