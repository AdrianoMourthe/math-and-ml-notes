Links to some important research papers or links. I plan to add notes as I go through each topic one by one.


## 1. Information theory based (unsupervised) learning
* [x] [Invariant Information Clustering](https://arxiv.org/abs/1807.06653)
   * [__Notes__](https://github.com/vinsis/math-and-ml-notes/blob/master/notes/iic.md)
* [x] [Mutual Information Neural Estimation](https://arxiv.org/abs/1801.04062)
  * [__Notes__](https://github.com/vinsis/math-and-ml-notes/blob/master/notes/mine.md)
* [x] [Deep Infomax](https://arxiv.org/abs/1808.06670)
  * [__Notes__](https://github.com/vinsis/math-and-ml-notes/blob/master/notes/deepinfomax.md)
* [x] [Learning Representations by Maximizing Mutual Information Across Views](https://arxiv.org/abs/1906.00910)
  * [__Notes__](https://github.com/vinsis/math-and-ml-notes/blob/master/notes/amdim.md)
* [x] How Google decoupled MI maximization and representation learning: [On Mutual Information Maximization for Representation Learning](https://arxiv.org/abs/1907.13625)
  * [__Notes__](https://github.com/vinsis/math-and-ml-notes/blob/master/notes/on_mi_maximization.md)

---

## 2. Disentangled representations
* [x] [Quick overview by Google](https://ai.googleblog.com/2019/04/evaluating-unsupervised-learning-of.html)
  *  [__Notes__](https://github.com/vinsis/math-and-ml-notes/blob/master/notes/unsupervised_disentanglement.md)
* [β-VAE, pdf](https://openreview.net/pdf?id=Sy2fzU9gl)
* [Understanding disentangling in β-VAE](https://arxiv.org/abs/1804.03599)
* [Disentangling Disentanglement in Variational Autoencoders](https://arxiv.org/abs/1812.02833)
* [Isolating Sources of Disentanglement in Variational Autoencoders](https://arxiv.org/abs/1802.04942)
* [InfoGAN-CR: Disentangling Generative Adversarial Networks with Contrastive Regularizers](https://arxiv.org/abs/1906.06034)
* [Disentangling by Factorising, PDF](https://www.cs.toronto.edu/~amnih/papers/disentangling_nips_ws.pdf)

---

## 3. Contrastive Coding
* [Data-Efficient Image Recognition with Contrastive Predictive Coding](https://arxiv.org/abs/1905.09272)
* [Contrastive Multiview Coding](https://arxiv.org/abs/1906.05849)
* [Momentum Contrast for Unsupervised Visual Representation Learning](https://arxiv.org/abs/1911.05722)
* Google disspelling a lot of misconceptions about disentangled representations: [Challenging Common Assumptions in the Unsupervised Learning of Disentangled Representations](https://arxiv.org/abs/1811.12359)

---

## 4. Automatic differentiation
* [x] [Automatic differentiation in machine learning: a survey](https://arxiv.org/abs/1502.05767)
* [x] [Automatic Reverse-Mode Differentiation: Lecture Notes](http://www.cs.cmu.edu/~wcohen/10-605/notes/autodiff.pdf)
  * [__Annotated pdf__](https://github.com/vinsis/math-and-ml-notes/blob/master/notes/autodiff.pdf)
* [x] [Reverse mode automatic differentiation](https://rufflewind.com/2016-12-30/reverse-mode-automatic-differentiation)
* [A new trick for calculating Jacobian vector products](https://j-towns.github.io/2017/06/12/A-new-trick.html)

---

## 5. NNs and ODEs
* [Beyond Finite Layer Neural Networks](https://arxiv.org/pdf/1710.10121.pdf)
* [Neural Ordinary Differential Equations](https://arxiv.org/pdf/1806.07366.pdf)
* [Augmented Neural ODEs](https://arxiv.org/abs/1904.01681)
* [Invertible ResNets](https://arxiv.org/pdf/1811.00995.pdf)

---

## 6. Miscellaneous
### Memorization in neural networks
* [Blog post by BAIR](https://bair.berkeley.edu/blog/2019/08/13/memorization/)
* [Identity Crisis: Paper](https://arxiv.org/abs/1902.04698)

### Probabilistic Programming
* [An introduction to probabilistic programming](https://arxiv.org/abs/1809.10756)

### Normalizing Flows
* [Detailed hands-on introduction](https://github.com/acids-ircam/pytorch_flows)
* [PyTorch implementations of density estimation algorithms](https://github.com/kamenbliznashki/normalizing_flows)

### Geometric deep learning
* [Good list of resources](http://geometricdeeplearning.com/)
* [Deep Convolutional Networks on Graph-Structured Data](https://arxiv.org/abs/1506.05163)

### Transformers
* [Attention is all you need](https://arxiv.org/abs/1706.03762)
* [Links to papers and code on various transformers by HuggingFace](https://github.com/huggingface/transformers)

### Others
* [Zero-shot knowledge transfer](https://arxiv.org/abs/1905.09768)
* [SpecNet](https://arxiv.org/abs/1905.10915)
* [Deep Learning & Symbolic Mathematics](https://arxiv.org/abs/1912.01412)

---

## 7. Theory of neural networks
### Lottery tickets
* [Lottery ticket hypothesis](http://news.mit.edu/2019/smarter-training-neural-networks-0506)
* [Deconstructing Lottery Tickets: Zeros, Signs, and the Supermask](https://arxiv.org/abs/1905.01067)
* [Rigging the Lottery: Making All Tickets Winners](https://arxiv.org/abs/1911.11134)

### Others
* [What's Hidden in a Randomly Weighted Neural Network?](https://arxiv.org/abs/1911.13299)
* [Topological properties of the set of functions generated by neural networks of fixed size](https://arxiv.org/abs/1806.08459)
* [YOUR  CLASSIFIER  IS  SECRETLY  AN  ENERGY  BASED MODEL AND YOU  SHOULD TREAT IT LIKE ONE](https://arxiv.org/abs/1912.03263)

---

## 8. Advanced Variational Inference 
* [Amortized Population Gibbs Samplers with Neural Sufficient Statistics](https://arxiv.org/abs/1911.01382)
* [Evaluating Combinatorial Generalization in Variational Autoencoders](https://arxiv.org/abs/1911.04594)

---
